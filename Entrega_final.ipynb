{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entrega_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvGJVVP5ep8NrOvHHoifn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancforero/Data-Science-UB-Capstone/blob/main/Entrega_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11X3Yiex1DV7"
      },
      "source": [
        "\n",
        "from oandapyV20 import API\n",
        "import oandapyV20.endpoints.trades as trades\n",
        "import oandapyV20.endpoints.accounts as accounts\n",
        "import oandapyV20.endpoints.instruments as instruments\n",
        "from oandapyV20.types import DateTime\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import mysql.connector\n",
        "import sqlalchemy\n",
        "\n",
        "\n",
        "#TOKEN generado directamente en la pagina de oanda para seguridad y nunero de cuenta\n",
        "api = API(access_token=\"xxxx\")\n",
        "accountID = \"xxxxx\"\n",
        "\n",
        "#detalles de la cuenta en general desde posiciones abiertas y saldo hasta contractuales como si se puede usar apalancamiento\n",
        "detalles = accounts.AccountDetails(accountID)\n",
        "\n",
        "#nombre de los activos que queremos descargar\n",
        "['FR40_EUR','EUR_GBP''EU50_EUR','DE30_EUR''EUR_USD','EUR_SGD','EUR_NOK','EUR_CAD','EUR_AUD','EUR_HKD','EUR_CHF']\n",
        "\n",
        "#\"imprime todos los instrumentos que puedo comprar y vender atra vez de oanda y por la API\n",
        "r = accounts.AccountInstruments(accountID=accountID)\n",
        "respuesta = api.request(r)\n",
        "universo_instrumentos = [respuesta['instruments'][i]['name']for i in range(0,len(respuesta['instruments'][:]))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AZt3yeu1eI-"
      },
      "source": [
        "### parametros para la consecucion de precios\n",
        "### vector donde guardaremos los precios optenidos dela api\n",
        "precios_organisados = np.zeros((10000000,13))\n",
        "fechas=[]\n",
        "#fecha inicial de donde empezaremos a descargar precios\n",
        "fecha_inicial = \"2019-01-23T12:00:00Z\"\n",
        "#categoria de los precios\n",
        "velas = ['bid','ask','mid']\n",
        "# o= open, h = hight, l= low, c=close\n",
        "tipos = ['o', 'h', 'l', 'c']\n",
        "#volumne= cantidad de operaciones ejecutadas en el intrevalo de la granularidad\n",
        "volumen = ['volume']\n",
        "#granularidad:intervalos entre los cuales queremos los precios M5:5 minute candlesticks\n",
        "granularidad = [\"M1\"]\n",
        "\n",
        "contador = 0\n",
        "try:\n",
        "    for i in range(0,10000):\n",
        "        if i > 0:\n",
        "            #maxima fecha en la que vamos a pedir datos\n",
        "            if fechas[-1][0:7] == \"2021-05\":\n",
        "                break\n",
        "            #la maxima cantidad de datos por la que se puede hacer la peticion al servidor es por 5000\n",
        "#            fecha_inicial = precios['candles'][-1]['time'].replace(\".000000000Z\",\".00Z\")\n",
        "            params = {\"count\": 5000,\"granularity\": granularidad,\"from\":fecha_inicial,\"price\":\"BAM\"}\n",
        "            pr = instruments.InstrumentsCandles(instrument=\"EUR_CHF\",params=params)\n",
        "            precios = api.request(pr)\n",
        "        else:\n",
        "            params = {\"count\": 5000,\"granularity\": granularidad,\"from\":fecha_inicial,\"price\":\"BAM\"}\n",
        "            pr = instruments.InstrumentsCandles(instrument=\"EUR_CHF\",params=params)\n",
        "            precios = api.request(pr)\n",
        "            \n",
        "        # recorremos cada uno de los precios para obtenere el precio maximo, minimo, medio del bid, ask y mid price   \n",
        "        for j in range(0,len(precios['candles'])-1):\n",
        "            fechas.append(precios['candles'][j]['time'])\n",
        "\n",
        "            k = 0\n",
        "            #recorremos todos el bid, ask y mid price\n",
        "            for vela in enumerate(velas):\n",
        "                #recorremos el precio de aperturaa,cierre, maximo y minimo\n",
        "                for tipo in enumerate(tipos):   \n",
        "                    precios_organisados[contador,k] = float(precios['candles'][j][vela[1]][tipo[1]])\n",
        "                    k= k+1\n",
        "\n",
        "            precios_organisados[contador,k] = float(precios['candles'][j]['volume'])\n",
        "            contador = contador + 1\n",
        "                \n",
        "        #toma la ultima fecha dela peticion para que sea la nueva fecha desde donde comienza la consulta.            \n",
        "        fecha_inicial = precios['candles'][-1]['time'].replace(\".000000000Z\",\".00Z\")\n",
        "        print(fecha_inicial)\n",
        "\n",
        "except:\n",
        "    print(\"no funciono\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2MwM5-21iMF"
      },
      "source": [
        "df_fechas = pd.DataFrame(fechas, columns=['FECHAS'])\n",
        "df_fechas.head(5)\n",
        "columnas=['BID_OPEN','BID_HIGH','BID_LOW','BID_CLOSE',\n",
        "          'ASK_OPEN','ASK_HIGH','ASK_LOW','ASK_CLOSE',\n",
        "          'MID_OPEN','MID_HIGH','MID_LOW','MID_CLOSE',\n",
        "          'VOLUME']\n",
        "#convertimos el arreglo de precios en un rata frame y ponemos nombres a las columnas\n",
        "df_precios_finales = pd.DataFrame(precios_organisados[0:len(fechas),:],columns=columnas)\n",
        "\n",
        "#unimos el dataFrame de fechas con el data frame de precios\n",
        "df_total = pd.concat([df_fechas,df_precios_finales],axis=1)\n",
        "df_total.head(10)\n",
        "\n",
        "\n",
        "#dado que ya tenemos los precios organizados en un dat frame, para no tener que volver a organizarlos\n",
        "#se guardara toda la informacion en un base de datas realacional.\n",
        "#abriumos la conexion con la base datos\n",
        "database_username = 'xxxx'\n",
        "database_password = 'xxxx'\n",
        "database_ip       = '127.0.0.1'\n",
        "database_name     = 'precios_activos'\n",
        "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
        "                                               format(database_username, database_password, \n",
        "                                                      database_ip, database_name), pool_recycle=1, pool_timeout=57600).connect()\n",
        "\n",
        "#creamos el objeto sql que sera subido a la base datos y donde crearesmo la nueva tabla\n",
        "df_total.to_sql(con=database_connection, name='EUR_CHF_5M',if_exists='replace', chunksize=5000) \n",
        "#cerramos la conexion\n",
        "database_connection.close()\n",
        "\n",
        "#grafico de la serie de precios    \n",
        "plt.plot(precios_organisados[0:len(fechas),2]) # plotting by columns\n",
        "plt.show()\n",
        "\n",
        "#calculo de los retornos logaritmicos de los precios y el grafico de la serie de retornos de una de las columas\n",
        "precios_organisados2 = precios_organisados[0:len(fechas),2]\n",
        "ret_precios =  np.log(precios_organisados2[40::]/precios_organisados2[0:-40])\n",
        "plt.plot(ret_precios) # plotting by columns\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#intervalos de los retornos\n",
        "q25, q75 = np.percentile(ret_precios,[.25,.75])\n",
        "bin_width = 2*(q75 - q25)*len(ret_precios)**(-1/3)\n",
        "bins = round((ret_precios.max() - ret_precios.min())/bin_width)\n",
        "\n",
        "# Histogram:\n",
        "# Bin it\n",
        "n, bin_edges = np.histogram(ret_precios, bins)\n",
        "# Normalize it, so that every bins value gives the probability of that bin\n",
        "bin_probability = n/float(n.sum())\n",
        "# Get the mid points of every bin\n",
        "bin_middles = (bin_edges[1:]+bin_edges[:-1])/2.\n",
        "# Compute the bin-width\n",
        "bin_width = bin_edges[1]-bin_edges[0]\n",
        "# Plot the histogram as a bar plot\n",
        "plt.bar(bin_middles, bin_probability, width=bin_width)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.xlim(-0.002,0.002)\n",
        "plt.ylim(0,0.016)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FJ5J3K11qWX"
      },
      "source": [
        "import sys\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import mysql.connector\n",
        "import sqlalchemy\n",
        "import ta as tt\n",
        "\n",
        "\n",
        "#conectarse a la base da datos para consultar las series de percios\n",
        "mydb = mysql.connector.connect(\n",
        "  host=\"localhost\",\n",
        "  user=\"xxx\",\n",
        "  password=\"xxx\",\n",
        "  database=\"precios_activos\"\n",
        ")\n",
        "\n",
        "#activod que se quieren consutar dentro de la base de datos\n",
        "series_activos = ['eur_usd_5m','eur_sgd_5m','eur_gbp_5m','eur_hkd_5m','eur_cad_5m']\n",
        "\n",
        "#query sql para acceder a los datos de las tablas  de la base de datos\n",
        "for activos in enumerate(series_activos):\n",
        "    mycursor = mydb.cursor()\n",
        "    \n",
        "    if activos[0] == 0:        \n",
        "        mycursor.execute(f\"SELECT FECHAS, BID_CLOSE, MID_CLOSE,ASK_CLOSE, VOLUME FROM {activos[1]}\")\n",
        "        myresult = mycursor.fetchall()\n",
        "        myresult = pd.DataFrame(myresult, columns=['FECHA',f\"BID_CLOSE_{activos[1]}\",f\"MID_CLOSE_{activos[1]}\",\n",
        "                                                    f\"BID_ASK_{activos[1]}\",f\"BID_CLOSE_{activos[1]}_VOLUME\"])\n",
        "    else:        \n",
        "        mycursor.execute(f\"SELECT  BID_CLOSE, MID_CLOSE, ASK_CLOSE, VOLUME FROM {activos[1]}\")\n",
        "        myresult2 = mycursor.fetchall()\n",
        "        myresult2 = pd.DataFrame(myresult2, columns=[f\"BID_CLOSE_{activos[1]}\",f\"MID_CLOSE_{activos[1]}\",\n",
        "                                                    f\"BID_ASK_{activos[1]}\",f\"BID_CLOSE_{activos[1]}_VOLUME\"])\n",
        "        myresult = pd.concat([myresult,myresult2], axis=1)\n",
        "\n",
        "\n",
        "bid_close_prices2 = myresult\n",
        "bid_close_prices3 = bid_close_prices2\n",
        "#escoger solo los datos de las series del bid\n",
        "bid_close_names =['BID_CLOSE_eur_usd_5m','BID_CLOSE_eur_sgd_5m',\n",
        "                              'BID_CLOSE_eur_gbp_5m','BID_CLOSE_eur_hkd_5m',\n",
        "                              'BID_CLOSE_eur_cad_5m']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxyiZwHJ-ft7"
      },
      "source": [
        "# calculo de indicadores tecnicos, en esta seccion se calcula el Exponential moving average con varias temporalidades\n",
        "windowSize =[5,10,15,15,20,25,30]\n",
        "for ew in enumerate(bid_close_names):\n",
        "    for window in windowSize:\n",
        "        bid_close_prices3[f\"{ew[1]}_EWMA{window}\"] = bid_close_prices3[f\"{ew[1]}\"].ewm(span=window, min_periods=window).mean()\n",
        "    \n",
        "    \n",
        "# en esta seccion se calcula las bandad de bollinger, el rsi y los promedio moviles\n",
        "bid_close=pd.DataFrame(bid_close_prices2['FECHA'])\n",
        "for bid in enumerate(bid_close_names):\n",
        "    bid_close[f\"{bid[1]}\"] = bid_close_prices2[f\"{bid[1]}\"]\n",
        "    for window in windowSize:\n",
        "        bid_close =pd.concat((bid_close,bid_close_prices3[f\"{bid[1]}_EWMA{window}\"]), axis=1)                 \n",
        "    bid_close[f\"{bid[1]}_volume\"] = bid_close_prices2[f\"{bid[1]}_VOLUME\"]\n",
        "    indicator_bb = tt.volatility.BollingerBands(bid_close_prices2[f\"{bid[1]}\"], window=20, window_dev=2)\n",
        "    bid_close[f\"{bid[1]}_moving_average\"]  = indicator_bb.bollinger_mavg()\n",
        "    bid_close[f\"{bid[1]}_bolbandhind\"]  =indicator_bb.bollinger_hband_indicator()\n",
        "    bid_close[f\"{bid[1]}_bolbandlind\"]  =indicator_bb.bollinger_lband_indicator()\n",
        "    bid_close[f\"{bid[1]}_bolbandh\"]  =indicator_bb.bollinger_hband()\n",
        "    bid_close[f\"{bid[1]}_bol_bandl\"]  =indicator_bb.bollinger_lband()\n",
        "    \n",
        "    RSI = tt.momentum.rsi(bid_close_prices2[f\"{bid[1]}\"], window=14, fillna=True)\n",
        "    bid_close[f\"{bid[1]}_RSI\"]  =RSI\n",
        "    \n",
        "    EMA = tt.trend.EMAIndicator(bid_close_prices2[f\"{bid[1]}\"], window = 14, fillna =True)\n",
        "    bid_close[f\"{bid[1]}_EMA\"]  =EMA.ema_indicator() \n",
        " \n",
        "#graficos de todas las series utulizadas\n",
        "plt.plot(bid_close_prices2['BID_CLOSE_eur_usd_5m'][1:1000000]) # plotting by columns\n",
        "plt.plot(bid_close_prices2['BID_CLOSE_eur_sgd_5m'][1:1000000]) # plotting by columns\n",
        "plt.plot(bid_close_prices2['BID_CLOSE_eur_gbp_5m'][1:1000000]) # plotting by columns\n",
        "plt.plot(bid_close_prices2['BID_CLOSE_eur_hkd_5m'][1:1000000]) # plotting by columns\n",
        "plt.plot(bid_close_prices2['BID_CLOSE_eur_cad_5m'][1:1000000]) # plotting by columns\n",
        "plt.show()\n",
        "\n",
        "bid_close3 =bid_close[850000::]\n",
        "\n",
        "#creacion de un archivo csv con las series nescesarias que vamos a utilizar despues en el analisis\n",
        "bid_close3.to_csv('path/TIME_SERIES_FINAL.CSV', sep=';', na_rep='', header=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-E75A57-l5j"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1ftubFs_Gz0"
      },
      "source": [
        "#informacion de la GPU que nos ha sido asiganda\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVBPn1bI_Ll6"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiU5OBKn_P56"
      },
      "source": [
        "#esta seccion tiene por objetuvo leer un archivo guardado directamente en google drive, apra este caso es el archivo que hemos generado arriba\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':'escribir id'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('TIME_SERIES_FINAL.CSV') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnn3gLcj_j76"
      },
      "source": [
        "#leer el archivo csv creado y convertirlo en un dataframa\n",
        "df2 = pd.read_csv('TIME_SERIES_FINAL.CSV',sep=';')\n",
        "df2= df2[30::]\n",
        "\n",
        "#observar los primeros 5 datos de la tabla\n",
        "df2.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtSzTvYI_xIL"
      },
      "source": [
        "#longitud de la tabla en filas\n",
        "print(len(df2))\n",
        "#ver el nombre de las columas\n",
        "df2.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWh6QmI7_9xR"
      },
      "source": [
        "#tomamos solo las columas que nos interesan para el analisis de la serue financiera del EUR_USD\n",
        "import numpy as np\n",
        "df = df2[['BID_CLOSE_eur_usd_5m','BID_CLOSE_eur_usd_5m_EWMA10','BID_CLOSE_eur_usd_5m_EWMA20','BID_CLOSE_eur_usd_5m_EWMA30','BID_CLOSE_eur_usd_5m_volume','BID_CLOSE_eur_usd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_usd_5m_bolbandhind','BID_CLOSE_eur_usd_5m_bolbandlind','BID_CLOSE_eur_usd_5m_bolbandh','BID_CLOSE_eur_usd_5m_bol_bandl','BID_CLOSE_eur_usd_5m_RSI','BID_CLOSE_eur_usd_5m_EMA',\n",
        "          'BID_CLOSE_eur_sgd_5m','BID_CLOSE_eur_sgd_5m_EWMA10','BID_CLOSE_eur_sgd_5m_EWMA20','BID_CLOSE_eur_sgd_5m_EWMA30','BID_CLOSE_eur_sgd_5m_volume','BID_CLOSE_eur_sgd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_sgd_5m_bolbandhind','BID_CLOSE_eur_sgd_5m_bolbandlind','BID_CLOSE_eur_sgd_5m_bolbandh','BID_CLOSE_eur_sgd_5m_bol_bandl','BID_CLOSE_eur_sgd_5m_RSI','BID_CLOSE_eur_sgd_5m_EMA',\n",
        "          'BID_CLOSE_eur_gbp_5m','BID_CLOSE_eur_gbp_5m_EWMA10','BID_CLOSE_eur_gbp_5m_EWMA20','BID_CLOSE_eur_gbp_5m_EWMA30','BID_CLOSE_eur_gbp_5m_volume','BID_CLOSE_eur_gbp_5m_moving_average',\n",
        "          'BID_CLOSE_eur_gbp_5m_bolbandhind','BID_CLOSE_eur_gbp_5m_bolbandlind','BID_CLOSE_eur_gbp_5m_bolbandh','BID_CLOSE_eur_gbp_5m_bol_bandl','BID_CLOSE_eur_gbp_5m_RSI','BID_CLOSE_eur_gbp_5m_EMA',\n",
        "          'BID_CLOSE_eur_hkd_5m','BID_CLOSE_eur_hkd_5m_EWMA10','BID_CLOSE_eur_hkd_5m_EWMA20','BID_CLOSE_eur_hkd_5m_EWMA30','BID_CLOSE_eur_hkd_5m_volume','BID_CLOSE_eur_hkd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_hkd_5m_bolbandhind','BID_CLOSE_eur_hkd_5m_bolbandlind','BID_CLOSE_eur_hkd_5m_bolbandh','BID_CLOSE_eur_hkd_5m_bol_bandl','BID_CLOSE_eur_hkd_5m_RSI','BID_CLOSE_eur_hkd_5m_EMA',\n",
        "          'BID_CLOSE_eur_cad_5m','BID_CLOSE_eur_cad_5m_EWMA10','BID_CLOSE_eur_cad_5m_EWMA20','BID_CLOSE_eur_cad_5m_EWMA30','BID_CLOSE_eur_cad_5m_volume','BID_CLOSE_eur_cad_5m_moving_average',\n",
        "          'BID_CLOSE_eur_cad_5m_bolbandhind','BID_CLOSE_eur_cad_5m_bolbandlind','BID_CLOSE_eur_cad_5m_bolbandh','BID_CLOSE_eur_cad_5m_bol_bandl','BID_CLOSE_eur_cad_5m_RSI','BID_CLOSE_eur_cad_5m_EMA']]\n",
        "\n",
        "df = df[80000:224000]\n",
        "df.reset_index(inplace=True)\n",
        "print(len(df))\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RRwq9xtAaV9"
      },
      "source": [
        "#observar la serie de datos\n",
        "plt.plot(df['BID_CLOSE_eur_usd_5m'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxIe_l-FAcUV"
      },
      "source": [
        "df = df[['BID_CLOSE_eur_usd_5m','BID_CLOSE_eur_usd_5m_EWMA10','BID_CLOSE_eur_usd_5m_EWMA20','BID_CLOSE_eur_usd_5m_EWMA30','BID_CLOSE_eur_usd_5m_volume','BID_CLOSE_eur_usd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_usd_5m_bolbandhind','BID_CLOSE_eur_usd_5m_bolbandlind','BID_CLOSE_eur_usd_5m_bolbandh','BID_CLOSE_eur_usd_5m_bol_bandl','BID_CLOSE_eur_usd_5m_RSI','BID_CLOSE_eur_usd_5m_EMA',\n",
        "          'BID_CLOSE_eur_sgd_5m','BID_CLOSE_eur_sgd_5m_EWMA10','BID_CLOSE_eur_sgd_5m_EWMA20','BID_CLOSE_eur_sgd_5m_EWMA30','BID_CLOSE_eur_sgd_5m_volume','BID_CLOSE_eur_sgd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_sgd_5m_bolbandhind','BID_CLOSE_eur_sgd_5m_bolbandlind','BID_CLOSE_eur_sgd_5m_bolbandh','BID_CLOSE_eur_sgd_5m_bol_bandl','BID_CLOSE_eur_sgd_5m_RSI','BID_CLOSE_eur_sgd_5m_EMA',\n",
        "          'BID_CLOSE_eur_gbp_5m','BID_CLOSE_eur_gbp_5m_EWMA10','BID_CLOSE_eur_gbp_5m_EWMA20','BID_CLOSE_eur_gbp_5m_EWMA30','BID_CLOSE_eur_gbp_5m_volume','BID_CLOSE_eur_gbp_5m_moving_average',\n",
        "          'BID_CLOSE_eur_gbp_5m_bolbandhind','BID_CLOSE_eur_gbp_5m_bolbandlind','BID_CLOSE_eur_gbp_5m_bolbandh','BID_CLOSE_eur_gbp_5m_bol_bandl','BID_CLOSE_eur_gbp_5m_RSI','BID_CLOSE_eur_gbp_5m_EMA',\n",
        "          'BID_CLOSE_eur_hkd_5m','BID_CLOSE_eur_hkd_5m_EWMA10','BID_CLOSE_eur_hkd_5m_EWMA20','BID_CLOSE_eur_hkd_5m_EWMA30','BID_CLOSE_eur_hkd_5m_volume','BID_CLOSE_eur_hkd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_hkd_5m_bolbandhind','BID_CLOSE_eur_hkd_5m_bolbandlind','BID_CLOSE_eur_hkd_5m_bolbandh','BID_CLOSE_eur_hkd_5m_bol_bandl','BID_CLOSE_eur_hkd_5m_RSI','BID_CLOSE_eur_hkd_5m_EMA',\n",
        "          'BID_CLOSE_eur_cad_5m','BID_CLOSE_eur_cad_5m_EWMA10','BID_CLOSE_eur_cad_5m_EWMA20','BID_CLOSE_eur_cad_5m_EWMA30','BID_CLOSE_eur_cad_5m_volume','BID_CLOSE_eur_cad_5m_moving_average',\n",
        "          'BID_CLOSE_eur_cad_5m_bolbandhind','BID_CLOSE_eur_cad_5m_bolbandlind','BID_CLOSE_eur_cad_5m_bolbandh','BID_CLOSE_eur_cad_5m_bol_bandl','BID_CLOSE_eur_cad_5m_RSI','BID_CLOSE_eur_cad_5m_EMA']]\n",
        "df_escalado = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1eBvXnSAfsU"
      },
      "source": [
        "# utilizamos el el paquete de sklearn para reescalr los datos para que queden entre cero y uno, dado que son series finacnieras no queremos precios negativos por dejabo de cero, \n",
        "#por lo cual consideramos conveniente no rescalar los datos al rededor del cero. luego haremos un edtudio con los datgo centrados alrededor del cero, utilizando retornos.\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "transformer = MaxAbsScaler().fit(df_escalado)\n",
        "transformer\n",
        "MaxAbsScaler()\n",
        "df = transformer.transform(df_escalado)\n",
        "\n",
        "columnas = ['BID_CLOSE_eur_usd_5m','BID_CLOSE_eur_usd_5m_EWMA10','BID_CLOSE_eur_usd_5m_EWMA20','BID_CLOSE_eur_usd_5m_EWMA30','BID_CLOSE_eur_usd_5m_volume','BID_CLOSE_eur_usd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_usd_5m_bolbandhind','BID_CLOSE_eur_usd_5m_bolbandlind','BID_CLOSE_eur_usd_5m_bolbandh','BID_CLOSE_eur_usd_5m_bol_bandl','BID_CLOSE_eur_usd_5m_RSI','BID_CLOSE_eur_usd_5m_EMA',\n",
        "          'BID_CLOSE_eur_sgd_5m','BID_CLOSE_eur_sgd_5m_EWMA10','BID_CLOSE_eur_sgd_5m_EWMA20','BID_CLOSE_eur_sgd_5m_EWMA30','BID_CLOSE_eur_sgd_5m_volume','BID_CLOSE_eur_sgd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_sgd_5m_bolbandhind','BID_CLOSE_eur_sgd_5m_bolbandlind','BID_CLOSE_eur_sgd_5m_bolbandh','BID_CLOSE_eur_sgd_5m_bol_bandl','BID_CLOSE_eur_sgd_5m_RSI','BID_CLOSE_eur_sgd_5m_EMA',\n",
        "          'BID_CLOSE_eur_gbp_5m','BID_CLOSE_eur_gbp_5m_EWMA10','BID_CLOSE_eur_gbp_5m_EWMA20','BID_CLOSE_eur_gbp_5m_EWMA30','BID_CLOSE_eur_gbp_5m_volume','BID_CLOSE_eur_gbp_5m_moving_average',\n",
        "          'BID_CLOSE_eur_gbp_5m_bolbandhind','BID_CLOSE_eur_gbp_5m_bolbandlind','BID_CLOSE_eur_gbp_5m_bolbandh','BID_CLOSE_eur_gbp_5m_bol_bandl','BID_CLOSE_eur_gbp_5m_RSI','BID_CLOSE_eur_gbp_5m_EMA',\n",
        "          'BID_CLOSE_eur_hkd_5m','BID_CLOSE_eur_hkd_5m_EWMA10','BID_CLOSE_eur_hkd_5m_EWMA20','BID_CLOSE_eur_hkd_5m_EWMA30','BID_CLOSE_eur_hkd_5m_volume','BID_CLOSE_eur_hkd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_hkd_5m_bolbandhind','BID_CLOSE_eur_hkd_5m_bolbandlind','BID_CLOSE_eur_hkd_5m_bolbandh','BID_CLOSE_eur_hkd_5m_bol_bandl','BID_CLOSE_eur_hkd_5m_RSI','BID_CLOSE_eur_hkd_5m_EMA',\n",
        "          'BID_CLOSE_eur_cad_5m','BID_CLOSE_eur_cad_5m_EWMA10','BID_CLOSE_eur_cad_5m_EWMA20','BID_CLOSE_eur_cad_5m_EWMA30','BID_CLOSE_eur_cad_5m_volume','BID_CLOSE_eur_cad_5m_moving_average',\n",
        "          'BID_CLOSE_eur_cad_5m_bolbandhind','BID_CLOSE_eur_cad_5m_bolbandlind','BID_CLOSE_eur_cad_5m_bolbandh','BID_CLOSE_eur_cad_5m_bol_bandl','BID_CLOSE_eur_cad_5m_RSI','BID_CLOSE_eur_cad_5m_EMA']\n",
        "df = pd.DataFrame(df,columns=columnas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uChKKsjEDCQ"
      },
      "source": [
        "#grafico descriptivo de las distribuciones de los activos\n",
        "sns.pairplot(df[[\"BID_CLOSE_eur_usd_5m\", \"BID_CLOSE_eur_sgd_5m\", \"BID_CLOSE_eur_gbp_5m\", \n",
        "                 'BID_CLOSE_eur_cad_5m',\"BID_CLOSE_eur_hkd_5m\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2-mIUu_EJxR"
      },
      "source": [
        "#estadsiticas descritpivas de las 5 principales series\n",
        "df_describe =df[[\"BID_CLOSE_eur_usd_5m\", \"BID_CLOSE_eur_sgd_5m\", \"BID_CLOSE_eur_gbp_5m\", \n",
        "                 'BID_CLOSE_eur_cad_5m',\"BID_CLOSE_eur_hkd_5m\"]]\n",
        "df_describe.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p4frOj5EPom"
      },
      "source": [
        "#grafico de las 5 series principales escaladas entre 0 y 1\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(label=\"Time Series\")\n",
        "plt.plot(df['BID_CLOSE_eur_usd_5m'],label=\"eur_usd\")\n",
        "plt.plot(df['BID_CLOSE_eur_sgd_5m'],label=\"eur_sgd\")  \n",
        "plt.plot(df['BID_CLOSE_eur_gbp_5m'],label=\"eur_gbp\")\n",
        "plt.plot(df['BID_CLOSE_eur_cad_5m'],label=\"eur_cad\")\n",
        "plt.plot(df['BID_CLOSE_eur_hkd_5m'],label=\"eur_hkd\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCoPf8ocEW0t"
      },
      "source": [
        "#grafico de los volumnes de negocion por tipo de activo\n",
        "plt.plot(df['BID_CLOSE_eur_usd_5m_volume'])\n",
        "plt.plot(df['BID_CLOSE_eur_sgd_5m_volume'])  \n",
        "plt.plot(df['BID_CLOSE_eur_gbp_5m_volume'])\n",
        "plt.plot(df['BID_CLOSE_eur_cad_5m_volume'])\n",
        "plt.plot(df['BID_CLOSE_eur_hkd_5m_volume'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ_k9SqjEysw"
      },
      "source": [
        "#estadistica descriptiva de todas slas varaibles otulizadas para el estudio\n",
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uUpoMbHE5od"
      },
      "source": [
        "#divisimos el set de datos general en train test y validation\n",
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]\n",
        "\n",
        "num_features = df.shape[1]\n",
        "\n",
        "\n",
        "print(n)\n",
        "print(num_features)\n",
        "print(len(train_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsUhBci1FI9u"
      },
      "source": [
        "#Grafico de los datos de ventrenamiento\n",
        "plt.plot(train_df['BID_CLOSE_eur_usd_5m'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_jdHplIGW8e"
      },
      "source": [
        "#funcion para crear los data sets con los coeficientes autorgresivos de la se la serie mas los de las otras series de tal manera que estene n un fromato legible para las redes.\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)        \n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNQnedAmGtAP"
      },
      "source": [
        "#variblle que nos indica cuantoas variables hacia atras en el tiempo queremos utilizar para predecir el siguiente vcalor\n",
        "time_steps = 15\n",
        "\n",
        "# reshape to [samples, time_steps, n_features]\n",
        "#creamos los datasets con las etiquetas para el entranimiento la validacion y el test del modelo\n",
        "X_train, y_train = create_dataset(train_df , train_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "X_test, y_test = create_dataset(val_df, val_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "X_val, y_val = create_dataset(test_df, test_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLyl-gWfHCOz"
      },
      "source": [
        "#dado que ya ya tenemos los ejemplos organizados con cada una imput por un autput, podemos mesclarlos de manera aleatoria para que el modelos no aprenda una tendencia en especifico\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZrPhKFdHfhO"
      },
      "source": [
        "#Funcion de compilacion del modelo, en este espeficamos la funcion de perdida el numero de epocas el optimizador y otras variables\n",
        "MAX_EPOCHS =100\n",
        "def compile_and_fit(model, X_train, y_train, y_test , patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "#  lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        " #    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "  optimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(loss=tf.keras.losses.Huber(),\n",
        "                optimizer=optimizer,\n",
        "                metrics=[\"mae\"])\n",
        "  history = model.fit(X_train, y_train, \n",
        "                      batch_size=64,\n",
        "#                      validation_data=y_test,\n",
        "                      validation_split=0.1,\n",
        "                      epochs=MAX_EPOCHS)#, callbacks=[lr_schedule])\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ariXR7JRMk"
      },
      "source": [
        "#modelo con capas LSTM \n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(900, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2]))),\n",
        "   tf.keras.layers.Dropout(0.2),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(600, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2]))),\n",
        "   tf.keras.layers.Dropout(0.2),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(350, return_sequences=False)),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp42EuH1JZp7"
      },
      "source": [
        "#compilar el modelo y guardar los resultados del modelo LSTM\n",
        "history = compile_and_fit(lstm_model, X_train, y_train, y_test)\n",
        "\n",
        "val_performance={}\n",
        "performance ={}\n",
        "IPython.display.clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Os_Jj8JgdD"
      },
      "source": [
        "#hacemos las predicciones en el conjunto de datoa de test y validacion para ver el performance del modelo\n",
        "y_pred = lstm_model.predict(X_test)\n",
        "y_pred_test = lstm_model.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o3rz4YmJza0"
      },
      "source": [
        "y_test_inv = y_test.reshape(1, -1)\n",
        "y_pred_inv = y_pred\n",
        "\n",
        "y_val_inv = y_val.reshape(1, -1)\n",
        "y_pred_inv_val = y_pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hl0PYgFNYHW"
      },
      "source": [
        "#grafico de prediccion de test\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(y_test_inv.flatten(), marker='.', label=\"true\")\n",
        "plt.plot(y_pred_inv.flatten(), 'r', label=\"prediction\")\n",
        "plt.xlim(10,28785)\n",
        "plt.ylim(0.940,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB0TOY3lNdIP"
      },
      "source": [
        "#grafico de validacion de del set de validacion\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(y_val_inv.flatten(), marker='.', label=\"true\")\n",
        "plt.plot(y_pred_inv_val.flatten(), 'r', label=\"prediction\")\n",
        "plt.xlim(10,14385)\n",
        "plt.ylim(0.940,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdn39yyxNgpr"
      },
      "source": [
        "#grafico de  la funciones de perdida y erro de entranamiento \n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "val_mae=history.history['val_mae']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.plot(epochs, val_mae, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\",[\"val_mae\"]])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2QfPkAIOCwU"
      },
      "source": [
        "#grafico de  la funciones de perdida y error de entranamiento \n",
        "zoomed_loss = loss[2:]\n",
        "zoomed_val_mae =val_mae[2:]\n",
        "zoomed_epochs = range(2,100)\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(zoomed_epochs, zoomed_loss, 'r',label=\"loss\")\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFjvypbXPiHM"
      },
      "source": [
        "#modelo convolucional completamnte conectado a una capas LSTM\n",
        "conv_model2 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=900, kernel_size=15,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[X_train.shape[1], X_train.shape[2]]),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(600, return_sequences=True)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(350, return_sequences=False)),\n",
        "    # Shape => [batch, time, features]\n",
        "  tf.keras.layers.Dense(units=1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVtMmgcJPqW7"
      },
      "source": [
        "#compilar el modelo y guardar los resultados del modelo LSTM\n",
        "history2 = compile_and_fit(conv_model2, X_train, y_train, y_test)\n",
        "\n",
        "val_performance={}\n",
        "performance ={}\n",
        "IPython.display.clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghk0Sm3pPyEi"
      },
      "source": [
        "#hacemos las predicciones en el conjunto de datoa de test y validacion para ver el performance del modelo\n",
        "y_pred = conv_model2.predict(X_test)\n",
        "y_pred_test = conv_model2.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXbX1U3BP7vC"
      },
      "source": [
        "#Predicciones en base a modelo convolucional\n",
        "y_pred_conv = conv_model2.predict(X_test)\n",
        "y_pred_test_conv = conv_model2.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81NrirREP9xg"
      },
      "source": [
        "#preparamos los datos que hemos predicho para hacer us graficas respectivas \n",
        "y_test_inv = y_test.reshape(1, -1)\n",
        "y_pred_inv = y_pred_conv\n",
        "y_val_inv = y_val.reshape(1, -1)\n",
        "y_pred_inv_val = y_pred_test_conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWSV_wtQLQ5"
      },
      "source": [
        "#grafico de prediccion de test\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(y_test_inv.flatten(), marker='.', label=\"true\")\n",
        "plt.plot(y_pred_inv.flatten(), 'r', label=\"prediction\")\n",
        "plt.xlim(10,28785)\n",
        "plt.ylim(0.940,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfgB5_2QQvP"
      },
      "source": [
        "#grafico de validacion de del set de validacion\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(y_val_inv.flatten(), marker='.', label=\"true\")\n",
        "plt.plot(y_pred_inv_val.flatten(), 'r', label=\"prediction\")\n",
        "plt.xlim(10,14385)\n",
        "plt.ylim(0.940,1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La8_ZkP3QS5C"
      },
      "source": [
        "#grafico de  la funciones de perdida y erro de entranamiento \n",
        "mae=history2.history['mae']\n",
        "loss=history2.history['loss']\n",
        "val_mae=history2.history['val_mae']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.plot(epochs, val_mae, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\",[\"val_mae\"]])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJeyp85UQX9M"
      },
      "source": [
        "#grafico de  la funciones de perdida y error de entranamiento \n",
        "zoomed_loss = loss[2:]\n",
        "zoomed_val_mae =val_mae[2:]\n",
        "zoomed_epochs = range(2,100)\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(zoomed_epochs, zoomed_loss, 'r',label=\"loss\")\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQd7OjEMUz-N"
      },
      "source": [
        "# MODELO DE CLARIFICACION DE LOS RETORNOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZdHUHldVGQf"
      },
      "source": [
        "#convertir el data frame en un arrray de numpy\n",
        "df = df.values\n",
        "df_ret=np.zeros((len(df[:-1,0]),len(df[0,:])))\n",
        "for i in range(0,len(df[0,:])):\n",
        "    df_ret[:,i] = np.log(df[1::,i]/df[0:-1,i])\n",
        "columnas =['BID_CLOSE_eur_usd_5m','BID_CLOSE_eur_usd_5m_EWMA10','BID_CLOSE_eur_usd_5m_EWMA20','BID_CLOSE_eur_usd_5m_EWMA30','BID_CLOSE_eur_usd_5m_volume','BID_CLOSE_eur_usd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_usd_5m_bolbandh','BID_CLOSE_eur_usd_5m_bol_bandl','BID_CLOSE_eur_usd_5m_RSI','BID_CLOSE_eur_usd_5m_EMA',\n",
        "          'BID_CLOSE_eur_sgd_5m','BID_CLOSE_eur_sgd_5m_EWMA10','BID_CLOSE_eur_sgd_5m_EWMA20','BID_CLOSE_eur_sgd_5m_EWMA30','BID_CLOSE_eur_sgd_5m_volume','BID_CLOSE_eur_sgd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_sgd_5m_bolbandh','BID_CLOSE_eur_sgd_5m_bol_bandl','BID_CLOSE_eur_sgd_5m_RSI','BID_CLOSE_eur_sgd_5m_EMA',\n",
        "          'BID_CLOSE_eur_gbp_5m','BID_CLOSE_eur_gbp_5m_EWMA10','BID_CLOSE_eur_gbp_5m_EWMA20','BID_CLOSE_eur_gbp_5m_EWMA30','BID_CLOSE_eur_gbp_5m_volume','BID_CLOSE_eur_gbp_5m_moving_average',\n",
        "          'BID_CLOSE_eur_gbp_5m_bolbandh','BID_CLOSE_eur_gbp_5m_bol_bandl','BID_CLOSE_eur_gbp_5m_RSI','BID_CLOSE_eur_gbp_5m_EMA',\n",
        "          'BID_CLOSE_eur_hkd_5m','BID_CLOSE_eur_hkd_5m_EWMA10','BID_CLOSE_eur_hkd_5m_EWMA20','BID_CLOSE_eur_hkd_5m_EWMA30','BID_CLOSE_eur_hkd_5m_volume','BID_CLOSE_eur_hkd_5m_moving_average',\n",
        "          'BID_CLOSE_eur_hkd_5m_bolbandh','BID_CLOSE_eur_hkd_5m_bol_bandl','BID_CLOSE_eur_hkd_5m_RSI','BID_CLOSE_eur_hkd_5m_EMA',\n",
        "          'BID_CLOSE_eur_cad_5m','BID_CLOSE_eur_cad_5m_EWMA10','BID_CLOSE_eur_cad_5m_EWMA20','BID_CLOSE_eur_cad_5m_EWMA30','BID_CLOSE_eur_cad_5m_volume','BID_CLOSE_eur_cad_5m_moving_average',\n",
        "          'BID_CLOSE_eur_cad_5m_bolbandh','BID_CLOSE_eur_cad_5m_bol_bandl','BID_CLOSE_eur_cad_5m_RSI','BID_CLOSE_eur_cad_5m_EMA']\n",
        "\n",
        "df = pd.DataFrame(df_ret, columns=columnas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xLUkC3VOY8"
      },
      "source": [
        "#GRAFICO CON LOS RETORSNO DE CADA UNA DE LAS SERIES PRINCIPALES\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(df['BID_CLOSE_eur_usd_5m'],'c',label='eur_usd')\n",
        "plt.legend()\n",
        "  \n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(df['BID_CLOSE_eur_sgd_5m'],'r',label='eur_sgd')\n",
        "plt.legend()\n",
        "  \n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(df['BID_CLOSE_eur_gbp_5m'],'m',label='eur_cgb')\n",
        "plt.ylim(-0.008,0.01)\n",
        "plt.legend()\n",
        "  \n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(df['BID_CLOSE_eur_cad_5m'],'g',label='eur_cad')\n",
        "plt.legend()\n",
        "plt.ylim(-0.006,0.008)\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(df['BID_CLOSE_eur_hkd_5m'],label='eur_hkd')\n",
        "plt.legend()  \n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgpmE1t0VWcR"
      },
      "source": [
        "#GRAFICO RETORNOS EUR_USD\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(label=\"Time Series\")\n",
        "plt.plot(df['BID_CLOSE_eur_usd_5m'],label=\"eur_usd\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCD3vcRZViAk"
      },
      "source": [
        "#grafico descriptivo de las distribuciones de los retornos de los activos\n",
        "sns.pairplot(df[[\"BID_CLOSE_eur_usd_5m\", \"BID_CLOSE_eur_sgd_5m\", \"BID_CLOSE_eur_gbp_5m\", \n",
        "                 'BID_CLOSE_eur_cad_5m',\"BID_CLOSE_eur_hkd_5m\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcc4-ixbVyKc"
      },
      "source": [
        "#estadsitica descritpiva de las series\n",
        "df_describe =df[[\"BID_CLOSE_eur_usd_5m\", \"BID_CLOSE_eur_sgd_5m\", \"BID_CLOSE_eur_gbp_5m\", \n",
        "                 'BID_CLOSE_eur_cad_5m',\"BID_CLOSE_eur_hkd_5m\"]]\n",
        "df_describe.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PXGPdEqV8Sf"
      },
      "source": [
        "#divisimos el set de datos general en train test y validation\n",
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]\n",
        "\n",
        "num_features = df.shape[1]\n",
        "\n",
        "\n",
        "print(n)\n",
        "print(num_features)\n",
        "print(len(train_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrLNyYvnWIDs"
      },
      "source": [
        "time_steps = 15\n",
        "\n",
        "# reshape to [samples, time_steps, n_features]\n",
        "\n",
        "X_train, y_train = create_dataset(train_df , train_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "X_test, y_test = create_dataset(val_df, val_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "X_val, y_val = create_dataset(test_df, test_df.BID_CLOSE_eur_usd_5m, time_steps)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHZq0xt5WjAw"
      },
      "source": [
        "#ctegorizacion de los intervalos para la clasificacion de los retornos\n",
        "y_train_oneh = np.where((y_train > 0.00006), 2, y_train)\n",
        "y_train_oneh = np.where(((y_train_oneh <= 0.00006) & (y_train_oneh >= -0.00003)), 1,y_train_oneh)\n",
        "y_train_oneh = np.where((y_train_oneh < -0.00003), 0, y_train_oneh)\n",
        "print(y_train_oneh[1:20])\n",
        "\n",
        "y_test_oneh = np.where((y_test > 0.00006), 2, y_test)\n",
        "y_test_oneh = np.where(((y_test_oneh <= 0.00006) & (y_test_oneh >= -0.00003)), 1,y_test_oneh)\n",
        "y_test_oneh = np.where((y_test_oneh < -0.00003), 0, y_test_oneh)\n",
        "print(y_test_oneh[1:20])\n",
        "\n",
        "y_val_oneh = np.where((y_val > 0.00006), 2, y_val)\n",
        "y_val_oneh = np.where(((y_val_oneh <= 0.00006) & (y_val_oneh>= -0.00003)), 1,y_val_oneh)\n",
        "y_val_oneh= np.where((y_val_oneh < -0.00003), 0, y_val_oneh)\n",
        "print(y_val_oneh[1:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8qfklMSWsCF"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "## convertrir los datos de prediccion en one hot enconders para poder generar un algoritmo de calsificacion\n",
        "\n",
        "#### train\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y_train_oneh)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_train_onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(y_train_onehot_encoded[1:10])\n",
        "\n",
        "#### test\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y_test_oneh)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_test_onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(y_test_onehot_encoded[1:10])\n",
        "\n",
        "#### validation\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y_val_oneh)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "y_val_onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(y_val_onehot_encoded[1:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHELmGziW18V"
      },
      "source": [
        "#Mesclar los ejemplode manera aleatoria \n",
        "from sklearn.utils import shuffle\n",
        "X_train,  y_train_onehot_encoded = shuffle(X_train, y_train_onehot_encoded, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbhbeUT_XBHj"
      },
      "source": [
        "#nueva funcion de compilacion del modelom teniendo en cuenta que ahora es problema de calisificacion y por lo tnaot utlizaramos por ejemplo una funcion objetivo crossentropy para un problema multiclase\n",
        "\n",
        "MAX_EPOCHS =70\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "def compile_and_fit(model, X_train, y_train, y_test , patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor=['val_loss','val_acc'],\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "#  lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        " #    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "  #optimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train, \n",
        "                      batch_size=64,\n",
        "#                      validation_data=(X_test, y_test),\n",
        "                      verbose=1,\n",
        "                      epochs=MAX_EPOCHS)#, callbacks=[lr_schedule])\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Bxvbk4ddrA"
      },
      "source": [
        "conv_model_clasi = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=128, kernel_size=15,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[X_train.shape[1], X_train.shape[2]]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(600, return_sequences=True)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(900, return_sequences=True)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(350, return_sequences=False)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "    # Shape => [batch, time, features]\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hudZXWOOekK_"
      },
      "source": [
        "ytete =np.argmax(y_test_onehot_encoded,axis=1)\n",
        "history3 = compile_and_fit(conv_model_clasi, X_train, y_train_onehot_encoded ,ytete)\n",
        "\n",
        "val_performance={}\n",
        "performance ={}\n",
        "IPython.display.clear_output()\n",
        "#val_performance['Conv'] = lstm_model.evaluate(y_train)\n",
        "#performance['Conv'] = lstm_model.evaluate(y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o62dd5mnerLm"
      },
      "source": [
        "#prediccion de la clasificacion en cada uno de los datao de\n",
        "y_pred_train_conv_clasi = conv_model_clasi.predict(X_train)\n",
        "y_pred_conv_clasi = conv_model_clasi.predict(X_test)\n",
        "y_pred_test_conv_clasi = conv_model_clasi.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6J4_2TAepaX"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(np.argmax(y_train_onehot_encoded,axis=1), np.argmax(y_pred_train_conv_clasi,axis=1))\n",
        "print(confusion)\n",
        "\n",
        "confusion1 = confusion_matrix(np.argmax(y_test_onehot_encoded,axis=1), np.argmax(y_pred_conv_clasi,axis=1))\n",
        "print(confusion1)\n",
        "\n",
        "confusion2 = confusion_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ4yy4WRfCjU"
      },
      "source": [
        "#matrices de confucion en version grafico\n",
        "import seaborn as sn\n",
        "M1 = np.array(confusion)\n",
        "\n",
        "M1\n",
        "df_cm = pd.DataFrame(M1,)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "# confusion_matrix(M1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMsqKxfnfH_n"
      },
      "source": [
        "#matrices de confucion en version grafico\n",
        "import seaborn as sn\n",
        "M2 = np.array(confusion2)\n",
        "\n",
        "M1\n",
        "df_cm = pd.DataFrame(M2,)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "# confusion_matrix(M1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9rioWRQfXWi"
      },
      "source": [
        "#grafico de la acuracy del modelo\n",
        "acurracy=history3.history['accuracy']\n",
        "loss=history3.history['loss']\n",
        "\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acurracy, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "\n",
        "plt.title('acurracy and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"acurracy\", \"Loss\",[\"val_mae\"]])\n",
        "\n",
        "plt.figure()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KC38I0RfdGk"
      },
      "source": [
        "zoomed_loss = loss[2:]\n",
        "\n",
        "zoomed_epochs = range(2,70)\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(zoomed_epochs, zoomed_loss, 'r',label=\"loss\")\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-x9Za82h7Ou"
      },
      "source": [
        "#modelos lstm de calsificacion\n",
        "lstm_model_clasi = tf.keras.models.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(900, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2]))),\n",
        "   tf.keras.layers.Dropout(0.2),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(600, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2]))),\n",
        "   tf.keras.layers.Dropout(0.2),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(350, return_sequences=False)),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxafTH-ZiFRF"
      },
      "source": [
        "ytete =np.argmax(y_test_onehot_encoded,axis=1)\n",
        "history4 = compile_and_fit(lstm_model_clasi, X_train, y_train_onehot_encoded ,ytete)\n",
        "\n",
        "val_performance={}\n",
        "performance ={}\n",
        "IPython.display.clear_output()\n",
        "#val_performance['Conv'] = lstm_model.evaluate(y_train)\n",
        "#performance['Conv'] = lstm_model.evaluate(y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YupU-gRkjqY8"
      },
      "source": [
        "#prediccion de la clasificacion en cada uno de los datao de\n",
        "y_pred_train_lstm_clasi = lstm_model_clasi.predict(X_train)\n",
        "y_pred_lstm_clasi = lstm_model_clasi.predict(X_test)\n",
        "y_pred_test_lstm_clasi = lstm_model_clasi.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_hGVYNmj4sO"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion3 = confusion_matrix(np.argmax(y_train_onehot_encoded,axis=1), np.argmax(y_pred_train_lstm_clasi,axis=1))\n",
        "print(confusion3)\n",
        "\n",
        "confusion4 = confusion_matrix(np.argmax(y_test_onehot_encoded,axis=1), np.argmax(y_pred_lstm_clasi,axis=1))\n",
        "print(confusion4)\n",
        "\n",
        "confusion2 = confusion_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG_rdZSOkbTj"
      },
      "source": [
        "#matrices de confucion en version grafico\n",
        "import seaborn as sn\n",
        "M3 = np.array(confusion3)\n",
        "\n",
        "M3\n",
        "df_cm = pd.DataFrame(M3,)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "# confusion_matrix(M1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LtnrNuUkf7z"
      },
      "source": [
        "#matrices de confucion en version grafico\n",
        "import seaborn as sn\n",
        "M4 = np.array(confusion4)\n",
        "\n",
        "M4\n",
        "df_cm = pd.DataFrame(M4,)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "# confusion_matrix(M1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYBWlGRTkqEr"
      },
      "source": [
        "#grafico de la acuracy del modelo\n",
        "acurracy=history4.history['accuracy']\n",
        "loss=history4.history['loss']\n",
        "\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acurracy, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "\n",
        "plt.title('acurracy and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"acurracy\", \"Loss\",[\"val_mae\"]])\n",
        "\n",
        "plt.figure()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu8iznLcku4s"
      },
      "source": [
        "zoomed_loss = loss[2:]\n",
        "\n",
        "zoomed_epochs = range(2,70)\n",
        "\n",
        "#------------------------------------------------\n",
        "plt.plot(zoomed_epochs, zoomed_loss, 'r',label=\"loss\")\n",
        "plt.title('Training loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\"])\n",
        "\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}